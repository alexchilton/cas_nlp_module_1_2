{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Transformer Model for Language Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Translate a PDF document from German to English\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:03.885717Z",
     "start_time": "2025-08-20T16:20:03.879816Z"
    }
   },
   "source": [
    "#!pip install -U spacy==3.7.2\n",
    "#!pip install -Uqq portalocker==2.7.0\n",
    "#!pip install -qq torchtext==0.14.1\n",
    "#!pip install -Uq nltk==3.8.1\n",
    "\n",
    "#!python -m spacy download de\n",
    "#!python -m spacy download en\n",
    "\n",
    "#!pip install pdfplumber==0.9.0\n",
    "#!pip install fpdf==1.7.2\n",
    "\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:06.913780Z",
     "start_time": "2025-08-20T16:20:04.031286Z"
    }
   },
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:09.736649Z",
     "start_time": "2025-08-20T16:20:06.928752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Access the data\n",
    "for example in train_data:\n",
    "    print(f\"English: {example['en']}\")\n",
    "    print(f\"German: {example['de']}\")\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Two young, White males are outside near many bushes.\n",
      "German: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:09.756681Z",
     "start_time": "2025-08-20T16:20:09.753609Z"
    }
   },
   "source": [
    "# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:11.728493Z",
     "start_time": "2025-08-20T16:20:09.825594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    \"\"\"Build vocabulary from sentences\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(sentence.split())\n",
    "\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def text_to_tensor(text, vocab, max_len=None):\n",
    "    \"\"\"Convert text to tensor using vocabulary\"\"\"\n",
    "    tokens = ['<bos>'] + text.split() + ['<eos>']\n",
    "    if max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "def get_translation_dataloaders_hf(batch_size=1, max_len=50):\n",
    "    \"\"\"\n",
    "    Replacement for TorchText's get_translation_dataloaders using Hugging Face Datasets\n",
    "    Returns tensors that can be transposed with .T\n",
    "    \"\"\"\n",
    "    # Load Multi30k dataset\n",
    "    dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "    # Get train and validation datasets\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['validation']\n",
    "\n",
    "    # Build vocabularies (you might want to save/load these)\n",
    "    print(\"Building vocabularies...\")\n",
    "    en_sentences = [item['en'] for item in train_dataset]\n",
    "    de_sentences = [item['de'] for item in train_dataset]\n",
    "\n",
    "    en_vocab = build_vocab(en_sentences)\n",
    "    de_vocab = build_vocab(de_sentences)\n",
    "\n",
    "    print(f\"English vocab size: {len(en_vocab)}\")\n",
    "    print(f\"German vocab size: {len(de_vocab)}\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function to convert text to tensors\"\"\"\n",
    "        english_tensors = []\n",
    "        german_tensors = []\n",
    "\n",
    "        # Find max length in batch for padding\n",
    "        max_en_len = max(len(item['en'].split()) + 2 for item in batch)  # +2 for <bos>, <eos>\n",
    "        max_de_len = max(len(item['de'].split()) + 2 for item in batch)\n",
    "\n",
    "        for item in batch:\n",
    "            en_tensor = text_to_tensor(item['en'], en_vocab, max_len)\n",
    "            de_tensor = text_to_tensor(item['de'], de_vocab, max_len)\n",
    "\n",
    "            # Pad to max length in batch\n",
    "            en_padded = torch.nn.functional.pad(en_tensor, (0, max_en_len - len(en_tensor)), value=en_vocab['<pad>'])\n",
    "            de_padded = torch.nn.functional.pad(de_tensor, (0, max_de_len - len(de_tensor)), value=de_vocab['<pad>'])\n",
    "\n",
    "            english_tensors.append(en_padded)\n",
    "            german_tensors.append(de_padded)\n",
    "\n",
    "        # Stack into batch tensors\n",
    "        english_batch = torch.stack(english_tensors)  # [batch_size, seq_len]\n",
    "        german_batch = torch.stack(german_tensors)    # [batch_size, seq_len]\n",
    "\n",
    "        return english_batch, german_batch\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Store vocabularies as attributes for later use\n",
    "    train_dataloader.en_vocab = en_vocab\n",
    "    train_dataloader.de_vocab = de_vocab\n",
    "    val_dataloader.en_vocab = en_vocab\n",
    "    val_dataloader.de_vocab = de_vocab\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Your replacement code:\n",
    "train_dataloader, val_dataloader = get_translation_dataloaders_hf(batch_size=1)\n",
    "\n",
    "# Create iterator\n",
    "data_itr = iter(train_dataloader)\n",
    "\n",
    "# Now this will work with tensors\n",
    "english, german = next(data_itr)\n",
    "print(f\"English tensor shape: {english.shape}\")\n",
    "print(f\"German tensor shape: {german.shape}\")\n",
    "\n",
    "# Now you can transpose!\n",
    "german = german.T\n",
    "english = english.T\n",
    "\n",
    "print(f\"After transpose - English: {english.shape}\")\n",
    "print(f\"After transpose - German: {german.shape}\")\n",
    "\n",
    "# Example: decode back to text to verify\n",
    "def decode_tensor(tensor, vocab):\n",
    "    \"\"\"Convert tensor back to text\"\"\"\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor.squeeze()]\n",
    "    # Remove padding and special tokens for display\n",
    "    words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "    return ' '.join(words)\n",
    "\n",
    "print(f\"English text: {decode_tensor(english, train_dataloader.en_vocab)}\")\n",
    "print(f\"German text: {decode_tensor(german, train_dataloader.de_vocab)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "English tensor shape: torch.Size([1, 17])\n",
      "German tensor shape: torch.Size([1, 15])\n",
      "After transpose - English: torch.Size([17, 1])\n",
      "After transpose - German: torch.Size([15, 1])\n",
      "English text: An elderly man sits outside a storefront accompanied by a young boy with a cart.\n",
      "German text: Ein älterer Mann sitzt mit einem Jungen mit einem Wagen vor einer Fassade.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.802943Z",
     "start_time": "2025-08-20T16:20:11.745193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Create simple iterator\n",
    "data_itr = iter(train_data)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.824469Z",
     "start_time": "2025-08-20T16:20:12.817994Z"
    }
   },
   "source": [
    "data_itr=iter(train_dataloader)\ndata_itr"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x166ac3aa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.930549Z",
     "start_time": "2025-08-20T16:20:12.864656Z"
    }
   },
   "source": [
    "for n in range(1000):\n    german, english= next(data_itr)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.941643Z",
     "start_time": "2025-08-20T16:20:12.939477Z"
    }
   },
   "source": [
    "german=german.T\nenglish=english.T"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.965004Z",
     "start_time": "2025-08-20T16:20:12.957207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def index_to_german(tensor, vocab=None):\n",
    "    \"\"\"Convert German tensor indices back to text\"\"\"\n",
    "    if vocab is None:\n",
    "        raise ValueError(\"Need German vocabulary to decode\")\n",
    "\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    if tensor.dim() > 1:\n",
    "        # Handle batch dimension\n",
    "        sentences = []\n",
    "        for i in range(tensor.shape[0]):\n",
    "            words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor[i]]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences\n",
    "    else:\n",
    "        words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor]\n",
    "        words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "        return ' '.join(words)\n",
    "\n",
    "def index_to_eng(tensor, vocab=None):\n",
    "    \"\"\"Convert English tensor indices back to text\"\"\"\n",
    "    if vocab is None:\n",
    "        raise ValueError(\"Need English vocabulary to decode\")\n",
    "\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    if tensor.dim() > 1:\n",
    "        # Handle batch dimension\n",
    "        sentences = []\n",
    "        for i in range(tensor.shape[0]):\n",
    "            words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor[i]]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences\n",
    "    else:\n",
    "        words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor]\n",
    "        words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Global variables to store vocabularies\n",
    "DE_VOCAB = None\n",
    "EN_VOCAB = None\n",
    "\n",
    "def set_global_vocabs(train_dataloader):\n",
    "    \"\"\"Set global vocabularies for easy access\"\"\"\n",
    "    global DE_VOCAB, EN_VOCAB\n",
    "    DE_VOCAB = train_dataloader.de_vocab\n",
    "    EN_VOCAB = train_dataloader.en_vocab\n",
    "\n",
    "def index_to_german_global(tensor):\n",
    "    \"\"\"Convert German tensor to text using global vocab\"\"\"\n",
    "    return index_to_german(tensor, DE_VOCAB)\n",
    "\n",
    "def index_to_eng_global(tensor):\n",
    "    \"\"\"Convert English tensor to text using global vocab\"\"\"\n",
    "    return index_to_eng(tensor, EN_VOCAB)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.662069Z",
     "start_time": "2025-08-20T16:20:12.989486Z"
    }
   },
   "source": [
    "train_dataloader, _ = get_translation_dataloaders_hf(batch_size=1)\n",
    "set_global_vocabs(train_dataloader)\n",
    "data_itr = iter(train_dataloader)\n",
    "\n",
    "\n",
    "for n in range(10):\n",
    "    german, english = next(data_itr)\n",
    "    print(\"sample {}\".format(n))\n",
    "    print(\"german input\")\n",
    "    print(index_to_german_global(german))\n",
    "    print(\"english target\")\n",
    "    print(index_to_eng_global(english))\n",
    "    print(\"_________\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "sample 0\n",
      "german input\n",
      "['klettert Rock <unk> Base Leiter ein Kampfsport. Haus']\n",
      "english target\n",
      "['giant goalie uniform pots is wooden gentleman boxes.']\n",
      "_________\n",
      "\n",
      "sample 1\n",
      "german input\n",
      "['Mädchen, liegt die schweißt. Tisches nicht, sitzt Autos, abzufangen,']\n",
      "english target\n",
      "['wearing hands hard notice pharmacy ladder do tulips tarp.']\n",
      "_________\n",
      "\n",
      "sample 2\n",
      "german input\n",
      "['klettert blickt blauen die ein sitzt beiden nicht, ein Hüte Motor schaukeln Kleinkind steigen. Ampel ihres']\n",
      "english target\n",
      "['giant am into hard frog an ladder painted blue <unk> <unk> color with mates']\n",
      "_________\n",
      "\n",
      "sample 3\n",
      "german input\n",
      "['klettert Reihe Jungen roten benutzt sind Sportwagen Leiter am sie']\n",
      "english target\n",
      "['purse row reading. <unk> with dancing bushes. large haircut. sand,']\n",
      "_________\n",
      "\n",
      "sample 4\n",
      "german input\n",
      "['sehen Motorhaube Bluse. ein senkrecht die in Sonnenschein stellt nicht, Hemd zum Bluse. Mehrere am <unk> 5']\n",
      "english target\n",
      "['in together bin men, walkway. lion grill. hard sign. desks do ladder man costume. outside <unk> office.']\n",
      "_________\n",
      "\n",
      "sample 5\n",
      "german input\n",
      "['klettert tragen zusieht. spielt, Holz zu. Ente nicht, Bei']\n",
      "english target\n",
      "['giant uniform young tables. In bushes ladder support']\n",
      "_________\n",
      "\n",
      "sample 6\n",
      "german input\n",
      "['klettert Viele <unk> langsam nicht, ein Mike blauen hinten eine Skifahrer stehen ein Torhüter nimmt fast']\n",
      "english target\n",
      "[\"purse instrument walking hard waves. ladder a motor into snowy puppet observes street. outside heritage McDonald's. stool Hockey\"]\n",
      "_________\n",
      "\n",
      "sample 7\n",
      "german input\n",
      "['klettert blauen Mehrere ein bedienen. etwas lächelt. Männer, Mehrere Freien. Jungen versucht, eine roter']\n",
      "english target\n",
      "['giant into many <unk> like white also slowly during ladder surrounded']\n",
      "_________\n",
      "\n",
      "sample 8\n",
      "german input\n",
      "['klettert kleine Mehrere roten läuft hält. ein Metallrahmen Mischung alle streut hoch, kleine Mehrere ein Hemd Artikel']\n",
      "english target\n",
      "['giant climbs hard reading. transportation. ladder wooden <unk> many bushes. catch alone. wooden running climbs many standing man container']\n",
      "_________\n",
      "\n",
      "sample 9\n",
      "german input\n",
      "['Zwei vieler Mehrere ein unaufgeräumten Löwen streut Mütze Eltern']\n",
      "english target\n",
      "['Two males many wooden shoulder. snowy chair']\n",
      "_________\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.679518Z",
     "start_time": "2025-08-20T16:20:14.676378Z"
    }
   },
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEVICE"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.714171Z",
     "start_time": "2025-08-20T16:20:14.711947Z"
    }
   },
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.748604Z",
     "start_time": "2025-08-20T16:20:14.744089Z"
    }
   },
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "The transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the tokens embeddings. These encodings have a fixed pattern based on their position in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.778430Z",
     "start_time": "2025-08-20T16:20:14.774557Z"
    }
   },
   "source": [
    "# Add positional information to the input tokens\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token embedding\n",
    "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
    "\n",
    "The `TokenEmbedding` class below converts numerical tokens into embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.805403Z",
     "start_time": "2025-08-20T16:20:14.800831Z"
    }
   },
   "source": [
    "class TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.827113Z",
     "start_time": "2025-08-20T16:20:14.822900Z"
    }
   },
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        outs =outs.to(DEVICE)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "The diagram below illustrates the sequence prediction or inference process.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/predict_transformers.png\" alt=\"transformer\">\n",
    "The decoder's output is then mapped onto a vocabulary-sized vector using a linear layer. Following this, a softmax function converts these vector scores into probabilities. The highest probability, as determined by the argmax function, provides the index of your predicted word within the translated sequence. This predicted index is fed back into the decoder in conjunction with the initial sequence, setting the stage to determine the subsequent word in the translation. This autoregressive process is demonstrated by the arrow pointing to form the top of the decoder, in green, to the bottom.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.850015Z",
     "start_time": "2025-08-20T16:20:14.847417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add this after the get_translation_dataloaders_hf function\n",
    "vocab_transform = {}\n",
    "\n",
    "def create_vocab_transform(train_dataloader):\n",
    "    \"\"\"Create vocab_transform dictionary for compatibility\"\"\"\n",
    "    global vocab_transform\n",
    "    vocab_transform = {\n",
    "        'de': train_dataloader.de_vocab,\n",
    "        'en': train_dataloader.en_vocab\n",
    "    }\n",
    "    return vocab_transform"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:16.648345Z",
     "start_time": "2025-08-20T16:20:14.869844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader, _ = get_translation_dataloaders_hf(batch_size=1)\n",
    "set_global_vocabs(train_dataloader)\n",
    "\n",
    "# Create vocab_transform for compatibility with existing code\n",
    "vocab_transform = create_vocab_transform(train_dataloader)\n",
    "\n",
    "# Now your existing code will work\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "\n",
    "print(f\"Source (German) vocab size: {SRC_VOCAB_SIZE}\")\n",
    "print(f\"Target (English) vocab size: {TGT_VOCAB_SIZE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "Source (German) vocab size: 9762\n",
      "Target (English) vocab size: 7964\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:16.963916Z",
     "start_time": "2025-08-20T16:20:16.664868Z"
    }
   },
   "source": [
    "torch.manual_seed(0)\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's will start off with a trained model.For this, load the weights of the transformer model from the file 'transformer.pt'.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:17.432430Z",
     "start_time": "2025-08-20T16:20:16.984062Z"
    }
   },
   "source": [
    "transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2SeqTransformer:\n\tsize mismatch for src_tok_emb.embedding.weight: copying a param with shape torch.Size([19214, 512]) from checkpoint, the shape in current model is torch.Size([9762, 512]).\n\tsize mismatch for tgt_tok_emb.embedding.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.bias: copying a param with shape torch.Size([10837]) from checkpoint, the shape in current model is torch.Size([7964]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtransformer.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2585\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2586\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2587\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2588\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2589\u001B[0m             ),\n\u001B[1;32m   2590\u001B[0m         )\n\u001B[1;32m   2592\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2593\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2594\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2595\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2596\u001B[0m         )\n\u001B[1;32m   2597\u001B[0m     )\n\u001B[1;32m   2598\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Seq2SeqTransformer:\n\tsize mismatch for src_tok_emb.embedding.weight: copying a param with shape torch.Size([19214, 512]) from checkpoint, the shape in current model is torch.Size([9762, 512]).\n\tsize mismatch for tgt_tok_emb.embedding.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.bias: copying a param with shape torch.Size([10837]) from checkpoint, the shape in current model is torch.Size([7964])."
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"engish target\",index_to_eng(tgt))\n",
    "#print(\"german input\",index_to_german(src))"
   ],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:51:12.527164Z",
     "start_time": "2025-08-20T18:51:11.917295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, let's check the vocabulary sizes from the checkpoint\n",
    "checkpoint = torch.load('transformer.pt', map_location=DEVICE)\n",
    "src_vocab_size = checkpoint['src_tok_emb.embedding.weight'].shape[0]\n",
    "tgt_vocab_size = checkpoint['tgt_tok_emb.embedding.weight'].shape[0]\n",
    "\n",
    "print(f\"Checkpoint expects - German vocab: {src_vocab_size}, English vocab: {tgt_vocab_size}\")\n",
    "\n",
    "# Create model with the checkpoint's expected vocabulary sizes\n",
    "SRC_VOCAB_SIZE = src_vocab_size  # 19214\n",
    "TGT_VOCAB_SIZE = tgt_vocab_size  # 10837\n",
    "EMB_SIZE = 512\n",
    "\n",
    "# Create the model architecture (you'll need your existing model definition)\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS,\n",
    "                               EMB_SIZE, NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "# Now load the weights\n",
    "transformer.load_state_dict(checkpoint)\n",
    "transformer.eval()\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint expects - German vocab: 19214, English vocab: 10837\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:53:06.679219Z",
     "start_time": "2025-08-20T18:53:06.334868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pdfplumber\n",
    "def extract_text_pdfplumber(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:53:24.292721Z",
     "start_time": "2025-08-20T18:53:24.271302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_for_translation(text):\n",
    "    # Split into sentences\n",
    "    sentences = text.split('.')\n",
    "    # Clean each sentence\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if sentence:  # Skip empty sentences\n",
    "            cleaned_sentences.append(sentence)\n",
    "    return cleaned_sentences"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:53:36.171512Z",
     "start_time": "2025-08-20T18:53:36.162371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_text(text, transformer, src_vocab, tgt_vocab, device):\n",
    "    # Tokenize text using your vocabulary\n",
    "    tokens = ['<bos>'] + text.split() + ['<eos>']\n",
    "    src_indices = [src_vocab.get(token, src_vocab['<unk>']) for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    # Generate translation using your transformer\n",
    "    with torch.no_grad():\n",
    "        # You'd need to implement the actual inference logic here\n",
    "        # This depends on how your transformer's forward method works\n",
    "        output = transformer.generate(src_tensor)  # This method would need to be implemented\n",
    "\n",
    "    # Convert output indices back to words\n",
    "    tgt_vocab_reverse = {v: k for k, v in tgt_vocab.items()}\n",
    "    translated_words = [tgt_vocab_reverse.get(idx.item(), '<unk>') for idx in output.squeeze()]\n",
    "\n",
    "    return ' '.join(translated_words)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:53:52.200242Z",
     "start_time": "2025-08-20T18:53:52.168475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_pdf(pdf_path, transformer, de_vocab, en_vocab, device):\n",
    "    # Extract text\n",
    "    text = extract_text_pdfplumber(pdf_path)\n",
    "\n",
    "    # Split into manageable chunks\n",
    "    sentences = preprocess_for_translation(text)\n",
    "\n",
    "    # Translate each sentence\n",
    "    translated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():\n",
    "            try:\n",
    "                translation = translate_text(sentence, transformer, de_vocab, en_vocab, device)\n",
    "                translated_sentences.append(translation)\n",
    "            except Exception as e:\n",
    "                print(f\"Error translating: {sentence[:50]}... Error: {e}\")\n",
    "                translated_sentences.append(f\"[TRANSLATION ERROR: {sentence}]\")\n",
    "\n",
    "    return '\\n'.join(translated_sentences)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T18:57:17.538546Z",
     "start_time": "2025-08-20T18:57:17.495697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check what methods your transformer has\n",
    "print(dir(transformer))\n",
    "\n",
    "# Or look for methods containing 'translate', 'generate', 'decode':\n",
    "methods = [method for method in dir(transformer) if any(word in method.lower() for word in ['translate', 'generate', 'decode', 'forward'])]\n",
    "print(\"Relevant methods:\", methods)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'cpu', 'cuda', 'decode', 'double', 'dump_patches', 'encode', 'eval', 'extra_repr', 'float', 'forward', 'generator', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'ipu', 'load_state_dict', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'positional_encoding', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'src_tok_emb', 'state_dict', 'tgt_tok_emb', 'to', 'to_empty', 'train', 'training', 'transformer', 'type', 'xpu', 'zero_grad']\n",
      "Relevant methods: ['_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_slow_forward', 'decode', 'forward', 'register_forward_hook', 'register_forward_pre_hook']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:07:11.267834Z",
     "start_time": "2025-08-20T19:07:11.237285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def translate_sentence_simple(sentence, transformer, de_vocab, en_vocab, device):\n",
    "    try:\n",
    "        # Convert to tokens\n",
    "        src_tokens = ['<bos>'] + sentence.split()[:10] + ['<eos>']\n",
    "        src_indices = [de_vocab.get(token, de_vocab['<unk>']) for token in src_tokens]\n",
    "\n",
    "        # Create tensors with sequence first [seq_len, batch_size]\n",
    "        src_tensor = torch.tensor(src_indices).unsqueeze(1).to(device)  # [seq_len, 1]\n",
    "        tgt_tensor = torch.tensor([en_vocab['<bos>']]).unsqueeze(1).to(device)  # [1, 1]\n",
    "\n",
    "        src_len = src_tensor.size(0)\n",
    "        tgt_len = tgt_tensor.size(0)\n",
    "\n",
    "        # Create masks\n",
    "        src_mask = torch.zeros((src_len, src_len), device=device)\n",
    "        tgt_mask = torch.zeros((tgt_len, tgt_len), device=device)\n",
    "        src_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "        tgt_padding_mask = torch.zeros((1, tgt_len), dtype=torch.bool, device=device)\n",
    "        memory_key_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            output = transformer(\n",
    "                src_tensor, tgt_tensor,\n",
    "                src_mask, tgt_mask,\n",
    "                src_padding_mask, tgt_padding_mask,\n",
    "                memory_key_padding_mask\n",
    "            )\n",
    "\n",
    "        # Get the most likely next token\n",
    "        probs = torch.softmax(output[-1, 0], dim=-1)  # Last position, first batch\n",
    "        next_token = torch.argmax(probs).item()\n",
    "\n",
    "        # Convert back to word\n",
    "        en_vocab_rev = {v: k for k, v in en_vocab.items()}\n",
    "        word = en_vocab_rev.get(next_token, '<unk>')\n",
    "\n",
    "        return f\"Predicted next word: {word}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)[:150]}\""
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:07:14.839904Z",
     "start_time": "2025-08-20T19:07:14.679371Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Replace in your PDF function:\n",
    "def translate_pdf_simple(pdf_path, transformer, de_vocab, en_vocab, device):\n",
    "    import pdfplumber\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \" \"\n",
    "\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    for i, sentence in enumerate(sentences[:3]):\n",
    "        if sentence.strip():\n",
    "            print(f\"German: {sentence.strip()}\")\n",
    "            translation = translate_sentence_simple(sentence.strip(), transformer, de_vocab, en_vocab, device)\n",
    "            print(f\"English: {translation}\")\n",
    "            print()\n",
    "\n",
    "# Use it:\n",
    "# Create vocabularies with the exact sizes the model expects\n",
    "def create_dummy_vocab(size, prefix=\"word\"):\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "    for i in range(4, size):\n",
    "        vocab[f\"{prefix}_{i}\"] = i\n",
    "    return vocab\n",
    "\n",
    "# Create the vocabularies\n",
    "de_vocab = create_dummy_vocab(19214, \"de\")  # German vocab\n",
    "en_vocab = create_dummy_vocab(10837, \"en\")  # English vocab\n",
    "\n",
    "# Now you can use them\n",
    "translate_pdf_simple('input_de.pdf', transformer, de_vocab, en_vocab, DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Der frühe Morgen bricht an und die ersten Sonnenstrahlen kitzeln san8 mein Gesicht\n",
      "English: Predicted next word: en_6\n",
      "\n",
      "German: Ich atme\n",
      "=ef ein und spüre die frische Morgenlu8 in meinen Lungen\n",
      "English: Predicted next word: en_6\n",
      "\n",
      "German: Mit einem Lächeln auf den Lippen\n",
      "stehe ich auf und beginne den Tag mit voller Energie\n",
      "English: Predicted next word: en_6\n",
      "\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:11:49.256222Z",
     "start_time": "2025-08-20T19:11:49.215636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def greedy_decode(transformer, src_tensor, src_mask, en_vocab, device, max_len=20):\n",
    "    \"\"\"Greedy decoding - always pick the most likely next word\"\"\"\n",
    "    generated_tokens = [en_vocab['<bos>']]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        tgt_tensor = torch.tensor(generated_tokens).unsqueeze(1).to(device)\n",
    "        tgt_len = tgt_tensor.size(0)\n",
    "        src_len = src_tensor.size(0)\n",
    "\n",
    "        # Create proper masks\n",
    "        src_mask = torch.zeros((src_len, src_len), device=device)\n",
    "        tgt_mask = torch.zeros((tgt_len, tgt_len), device=device)  # Fixed: use tgt_len\n",
    "        src_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "        tgt_padding_mask = torch.zeros((1, tgt_len), dtype=torch.bool, device=device)\n",
    "        memory_key_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = transformer(src_tensor, tgt_tensor, src_mask, tgt_mask,\n",
    "                               src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "        # Greedy: pick the most likely token\n",
    "        probs = torch.softmax(output[-1, 0], dim=-1)\n",
    "        next_token = torch.argmax(probs).item()\n",
    "\n",
    "        if next_token == en_vocab['<eos>']:\n",
    "            break\n",
    "\n",
    "        generated_tokens.append(next_token)\n",
    "\n",
    "    return generated_tokens[1:]  # Remove <bos>\n",
    "\n",
    "def beam_search(transformer, src_tensor, src_mask, en_vocab, device, beam_size=3, max_len=20):\n",
    "    \"\"\"Beam search - keep track of top K sequences\"\"\"\n",
    "    beams = [([en_vocab['<bos>']], 0.0)]\n",
    "\n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "\n",
    "        for sequence, score in beams:\n",
    "            if sequence[-1] == en_vocab['<eos>']:\n",
    "                candidates.append((sequence, score))\n",
    "                continue\n",
    "\n",
    "            tgt_tensor = torch.tensor(sequence).unsqueeze(1).to(device)\n",
    "            tgt_len = tgt_tensor.size(0)\n",
    "            src_len = src_tensor.size(0)\n",
    "\n",
    "            # Create proper masks\n",
    "            src_mask_local = torch.zeros((src_len, src_len), device=device)\n",
    "            tgt_mask = torch.zeros((tgt_len, tgt_len), device=device)  # Fixed\n",
    "            src_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "            tgt_padding_mask = torch.zeros((1, tgt_len), dtype=torch.bool, device=device)\n",
    "            memory_key_padding_mask = torch.zeros((1, src_len), dtype=torch.bool, device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = transformer(src_tensor, tgt_tensor, src_mask_local, tgt_mask,\n",
    "                                   src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "            # Get top beam_size tokens\n",
    "            log_probs = torch.log_softmax(output[-1, 0], dim=-1)\n",
    "            top_probs, top_indices = torch.topk(log_probs, beam_size)\n",
    "\n",
    "            for prob, idx in zip(top_probs, top_indices):\n",
    "                new_sequence = sequence + [idx.item()]\n",
    "                new_score = score + prob.item()\n",
    "                candidates.append((new_sequence, new_score))\n",
    "\n",
    "        # Keep only top beam_size sequences\n",
    "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "        # Check if all beams ended\n",
    "        if all(seq[-1] == en_vocab['<eos>'] for seq, _ in beams):\n",
    "            break\n",
    "\n",
    "    best_sequence, best_score = beams[0]\n",
    "    return best_sequence[1:]  # Remove <bos>\n",
    "\n",
    "def compare_decoding_methods(sentence, transformer, de_vocab, en_vocab, device):\n",
    "    \"\"\"Compare greedy vs beam search for a sentence\"\"\"\n",
    "    try:\n",
    "        # Prepare source\n",
    "        src_tokens = ['<bos>'] + sentence.split()[:8] + ['<eos>']\n",
    "        src_indices = [de_vocab.get(token, de_vocab['<unk>']) for token in src_tokens]\n",
    "        src_tensor = torch.tensor(src_indices).unsqueeze(1).to(device)\n",
    "        src_mask = None  # Let the functions create their own masks\n",
    "\n",
    "        print(f\"German: {sentence}\")\n",
    "\n",
    "        # Greedy decoding\n",
    "        greedy_tokens = greedy_decode(transformer, src_tensor, src_mask, en_vocab, device)\n",
    "        en_vocab_rev = {v: k for k, v in en_vocab.items()}\n",
    "        greedy_words = [en_vocab_rev.get(token, f'token_{token}') for token in greedy_tokens]\n",
    "        print(f\"Greedy:     {' '.join(greedy_words)}\")\n",
    "\n",
    "        # Beam search\n",
    "        beam_tokens = beam_search(transformer, src_tensor, src_mask, en_vocab, device, beam_size=3)\n",
    "        beam_words = [en_vocab_rev.get(token, f'token_{token}') for token in beam_tokens]\n",
    "        print(f\"Beam(k=3):  {' '.join(beam_words)}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:100]}\")"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T19:11:54.834571Z",
     "start_time": "2025-08-20T19:11:52.748896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Usage:\n",
    "sentences = [\n",
    "    \"Der frühe Morgen bricht an\",\n",
    "    \"Ich gehe zur Schule\",\n",
    "    \"Das Wetter ist schön\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    compare_decoding_methods(sentence, transformer, de_vocab, en_vocab, DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Der frühe Morgen bricht an\n",
      "Greedy:     en_6 en_193 en_966 en_10 en_199 en_26 en_1365 en_5\n",
      "Beam(k=3):  en_6 en_193 en_966 en_14 en_26 en_1424 <eos>\n",
      "--------------------------------------------------\n",
      "German: Ich gehe zur Schule\n",
      "Greedy:     en_6 en_193 en_966 en_10 en_199 en_26 en_1365 en_5\n",
      "Beam(k=3):  en_6 en_193 en_966 en_14 en_26 en_1424 <eos>\n",
      "--------------------------------------------------\n",
      "German: Das Wetter ist schön\n",
      "Greedy:     en_6 en_193 en_966 en_10 en_199 en_26 en_1365 en_5\n",
      "Beam(k=3):  en_6 en_193 en_966 en_14 en_26 en_1424 <eos>\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 45
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "f583ab330d392f3fbc803e1d84830f575a94e0d7cc0f8b3af49ded45fd51cc14"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
