{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Transformer Model for Language Translation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "After completing this lab, you will be able to:\n",
    "\n",
    "- Translate a PDF document from German to English\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:03.885717Z",
     "start_time": "2025-08-20T16:20:03.879816Z"
    }
   },
   "source": [
    "#!pip install -U spacy==3.7.2\n",
    "#!pip install -Uqq portalocker==2.7.0\n",
    "#!pip install -qq torchtext==0.14.1\n",
    "#!pip install -Uq nltk==3.8.1\n",
    "\n",
    "#!python -m spacy download de\n",
    "#!python -m spacy download en\n",
    "\n",
    "#!pip install pdfplumber==0.9.0\n",
    "#!pip install fpdf==1.7.2\n",
    "\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/transformer.pt'\n",
    "#!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/input_de.pdf'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:06.913780Z",
     "start_time": "2025-08-20T16:20:04.031286Z"
    }
   },
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "# You can also use this section to suppress warnings generated by your code:\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:09.736649Z",
     "start_time": "2025-08-20T16:20:06.928752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Access the data\n",
    "for example in train_data:\n",
    "    print(f\"English: {example['en']}\")\n",
    "    print(f\"German: {example['de']}\")\n",
    "    break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Two young, White males are outside near many bushes.\n",
      "German: Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:09.756681Z",
     "start_time": "2025-08-20T16:20:09.753609Z"
    }
   },
   "source": [
    "# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:11.728493Z",
     "start_time": "2025-08-20T16:20:09.825594Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def build_vocab(sentences, min_freq=2):\n",
    "    \"\"\"Build vocabulary from sentences\"\"\"\n",
    "    counter = Counter()\n",
    "    for sentence in sentences:\n",
    "        counter.update(sentence.split())\n",
    "\n",
    "    vocab = {'<unk>': 0, '<pad>': 1, '<bos>': 2, '<eos>': 3}\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab\n",
    "\n",
    "def text_to_tensor(text, vocab, max_len=None):\n",
    "    \"\"\"Convert text to tensor using vocabulary\"\"\"\n",
    "    tokens = ['<bos>'] + text.split() + ['<eos>']\n",
    "    if max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "def get_translation_dataloaders_hf(batch_size=1, max_len=50):\n",
    "    \"\"\"\n",
    "    Replacement for TorchText's get_translation_dataloaders using Hugging Face Datasets\n",
    "    Returns tensors that can be transposed with .T\n",
    "    \"\"\"\n",
    "    # Load Multi30k dataset\n",
    "    dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "    # Get train and validation datasets\n",
    "    train_dataset = dataset['train']\n",
    "    val_dataset = dataset['validation']\n",
    "\n",
    "    # Build vocabularies (you might want to save/load these)\n",
    "    print(\"Building vocabularies...\")\n",
    "    en_sentences = [item['en'] for item in train_dataset]\n",
    "    de_sentences = [item['de'] for item in train_dataset]\n",
    "\n",
    "    en_vocab = build_vocab(en_sentences)\n",
    "    de_vocab = build_vocab(de_sentences)\n",
    "\n",
    "    print(f\"English vocab size: {len(en_vocab)}\")\n",
    "    print(f\"German vocab size: {len(de_vocab)}\")\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        \"\"\"Custom collate function to convert text to tensors\"\"\"\n",
    "        english_tensors = []\n",
    "        german_tensors = []\n",
    "\n",
    "        # Find max length in batch for padding\n",
    "        max_en_len = max(len(item['en'].split()) + 2 for item in batch)  # +2 for <bos>, <eos>\n",
    "        max_de_len = max(len(item['de'].split()) + 2 for item in batch)\n",
    "\n",
    "        for item in batch:\n",
    "            en_tensor = text_to_tensor(item['en'], en_vocab, max_len)\n",
    "            de_tensor = text_to_tensor(item['de'], de_vocab, max_len)\n",
    "\n",
    "            # Pad to max length in batch\n",
    "            en_padded = torch.nn.functional.pad(en_tensor, (0, max_en_len - len(en_tensor)), value=en_vocab['<pad>'])\n",
    "            de_padded = torch.nn.functional.pad(de_tensor, (0, max_de_len - len(de_tensor)), value=de_vocab['<pad>'])\n",
    "\n",
    "            english_tensors.append(en_padded)\n",
    "            german_tensors.append(de_padded)\n",
    "\n",
    "        # Stack into batch tensors\n",
    "        english_batch = torch.stack(english_tensors)  # [batch_size, seq_len]\n",
    "        german_batch = torch.stack(german_tensors)    # [batch_size, seq_len]\n",
    "\n",
    "        return english_batch, german_batch\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    # Store vocabularies as attributes for later use\n",
    "    train_dataloader.en_vocab = en_vocab\n",
    "    train_dataloader.de_vocab = de_vocab\n",
    "    val_dataloader.en_vocab = en_vocab\n",
    "    val_dataloader.de_vocab = de_vocab\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "# Your replacement code:\n",
    "train_dataloader, val_dataloader = get_translation_dataloaders_hf(batch_size=1)\n",
    "\n",
    "# Create iterator\n",
    "data_itr = iter(train_dataloader)\n",
    "\n",
    "# Now this will work with tensors\n",
    "english, german = next(data_itr)\n",
    "print(f\"English tensor shape: {english.shape}\")\n",
    "print(f\"German tensor shape: {german.shape}\")\n",
    "\n",
    "# Now you can transpose!\n",
    "german = german.T\n",
    "english = english.T\n",
    "\n",
    "print(f\"After transpose - English: {english.shape}\")\n",
    "print(f\"After transpose - German: {german.shape}\")\n",
    "\n",
    "# Example: decode back to text to verify\n",
    "def decode_tensor(tensor, vocab):\n",
    "    \"\"\"Convert tensor back to text\"\"\"\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor.squeeze()]\n",
    "    # Remove padding and special tokens for display\n",
    "    words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "    return ' '.join(words)\n",
    "\n",
    "print(f\"English text: {decode_tensor(english, train_dataloader.en_vocab)}\")\n",
    "print(f\"German text: {decode_tensor(german, train_dataloader.de_vocab)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "English tensor shape: torch.Size([1, 17])\n",
      "German tensor shape: torch.Size([1, 15])\n",
      "After transpose - English: torch.Size([17, 1])\n",
      "After transpose - German: torch.Size([15, 1])\n",
      "English text: An elderly man sits outside a storefront accompanied by a young boy with a cart.\n",
      "German text: Ein älterer Mann sitzt mit einem Jungen mit einem Wagen vor einer Fassade.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.802943Z",
     "start_time": "2025-08-20T16:20:11.745193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Create simple iterator\n",
    "data_itr = iter(train_data)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.824469Z",
     "start_time": "2025-08-20T16:20:12.817994Z"
    }
   },
   "source": [
    "data_itr=iter(train_dataloader)\ndata_itr"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x166ac3aa0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.930549Z",
     "start_time": "2025-08-20T16:20:12.864656Z"
    }
   },
   "source": [
    "for n in range(1000):\n    german, english= next(data_itr)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.941643Z",
     "start_time": "2025-08-20T16:20:12.939477Z"
    }
   },
   "source": [
    "german=german.T\nenglish=english.T"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:12.965004Z",
     "start_time": "2025-08-20T16:20:12.957207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def index_to_german(tensor, vocab=None):\n",
    "    \"\"\"Convert German tensor indices back to text\"\"\"\n",
    "    if vocab is None:\n",
    "        raise ValueError(\"Need German vocabulary to decode\")\n",
    "\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    if tensor.dim() > 1:\n",
    "        # Handle batch dimension\n",
    "        sentences = []\n",
    "        for i in range(tensor.shape[0]):\n",
    "            words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor[i]]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences\n",
    "    else:\n",
    "        words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor]\n",
    "        words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "        return ' '.join(words)\n",
    "\n",
    "def index_to_eng(tensor, vocab=None):\n",
    "    \"\"\"Convert English tensor indices back to text\"\"\"\n",
    "    if vocab is None:\n",
    "        raise ValueError(\"Need English vocabulary to decode\")\n",
    "\n",
    "    idx_to_word = {v: k for k, v in vocab.items()}\n",
    "    if tensor.dim() > 1:\n",
    "        # Handle batch dimension\n",
    "        sentences = []\n",
    "        for i in range(tensor.shape[0]):\n",
    "            words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor[i]]\n",
    "            # Remove special tokens and padding\n",
    "            words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "            sentences.append(' '.join(words))\n",
    "        return sentences\n",
    "    else:\n",
    "        words = [idx_to_word.get(idx.item(), '<unk>') for idx in tensor]\n",
    "        words = [w for w in words if w not in ['<pad>', '<bos>', '<eos>']]\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Global variables to store vocabularies\n",
    "DE_VOCAB = None\n",
    "EN_VOCAB = None\n",
    "\n",
    "def set_global_vocabs(train_dataloader):\n",
    "    \"\"\"Set global vocabularies for easy access\"\"\"\n",
    "    global DE_VOCAB, EN_VOCAB\n",
    "    DE_VOCAB = train_dataloader.de_vocab\n",
    "    EN_VOCAB = train_dataloader.en_vocab\n",
    "\n",
    "def index_to_german_global(tensor):\n",
    "    \"\"\"Convert German tensor to text using global vocab\"\"\"\n",
    "    return index_to_german(tensor, DE_VOCAB)\n",
    "\n",
    "def index_to_eng_global(tensor):\n",
    "    \"\"\"Convert English tensor to text using global vocab\"\"\"\n",
    "    return index_to_eng(tensor, EN_VOCAB)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.662069Z",
     "start_time": "2025-08-20T16:20:12.989486Z"
    }
   },
   "source": [
    "train_dataloader, _ = get_translation_dataloaders_hf(batch_size=1)\n",
    "set_global_vocabs(train_dataloader)\n",
    "data_itr = iter(train_dataloader)\n",
    "\n",
    "\n",
    "for n in range(10):\n",
    "    german, english = next(data_itr)\n",
    "    print(\"sample {}\".format(n))\n",
    "    print(\"german input\")\n",
    "    print(index_to_german_global(german))\n",
    "    print(\"english target\")\n",
    "    print(index_to_eng_global(english))\n",
    "    print(\"_________\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "sample 0\n",
      "german input\n",
      "['klettert Rock <unk> Base Leiter ein Kampfsport. Haus']\n",
      "english target\n",
      "['giant goalie uniform pots is wooden gentleman boxes.']\n",
      "_________\n",
      "\n",
      "sample 1\n",
      "german input\n",
      "['Mädchen, liegt die schweißt. Tisches nicht, sitzt Autos, abzufangen,']\n",
      "english target\n",
      "['wearing hands hard notice pharmacy ladder do tulips tarp.']\n",
      "_________\n",
      "\n",
      "sample 2\n",
      "german input\n",
      "['klettert blickt blauen die ein sitzt beiden nicht, ein Hüte Motor schaukeln Kleinkind steigen. Ampel ihres']\n",
      "english target\n",
      "['giant am into hard frog an ladder painted blue <unk> <unk> color with mates']\n",
      "_________\n",
      "\n",
      "sample 3\n",
      "german input\n",
      "['klettert Reihe Jungen roten benutzt sind Sportwagen Leiter am sie']\n",
      "english target\n",
      "['purse row reading. <unk> with dancing bushes. large haircut. sand,']\n",
      "_________\n",
      "\n",
      "sample 4\n",
      "german input\n",
      "['sehen Motorhaube Bluse. ein senkrecht die in Sonnenschein stellt nicht, Hemd zum Bluse. Mehrere am <unk> 5']\n",
      "english target\n",
      "['in together bin men, walkway. lion grill. hard sign. desks do ladder man costume. outside <unk> office.']\n",
      "_________\n",
      "\n",
      "sample 5\n",
      "german input\n",
      "['klettert tragen zusieht. spielt, Holz zu. Ente nicht, Bei']\n",
      "english target\n",
      "['giant uniform young tables. In bushes ladder support']\n",
      "_________\n",
      "\n",
      "sample 6\n",
      "german input\n",
      "['klettert Viele <unk> langsam nicht, ein Mike blauen hinten eine Skifahrer stehen ein Torhüter nimmt fast']\n",
      "english target\n",
      "[\"purse instrument walking hard waves. ladder a motor into snowy puppet observes street. outside heritage McDonald's. stool Hockey\"]\n",
      "_________\n",
      "\n",
      "sample 7\n",
      "german input\n",
      "['klettert blauen Mehrere ein bedienen. etwas lächelt. Männer, Mehrere Freien. Jungen versucht, eine roter']\n",
      "english target\n",
      "['giant into many <unk> like white also slowly during ladder surrounded']\n",
      "_________\n",
      "\n",
      "sample 8\n",
      "german input\n",
      "['klettert kleine Mehrere roten läuft hält. ein Metallrahmen Mischung alle streut hoch, kleine Mehrere ein Hemd Artikel']\n",
      "english target\n",
      "['giant climbs hard reading. transportation. ladder wooden <unk> many bushes. catch alone. wooden running climbs many standing man container']\n",
      "_________\n",
      "\n",
      "sample 9\n",
      "german input\n",
      "['Zwei vieler Mehrere ein unaufgeräumten Löwen streut Mütze Eltern']\n",
      "english target\n",
      "['Two males many wooden shoulder. snowy chair']\n",
      "_________\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.679518Z",
     "start_time": "2025-08-20T16:20:14.676378Z"
    }
   },
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEVICE"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.714171Z",
     "start_time": "2025-08-20T16:20:14.711947Z"
    }
   },
   "source": [
    "def generate_square_subsequent_mask(sz,device=DEVICE):\n    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n    return mask"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.748604Z",
     "start_time": "2025-08-20T16:20:14.744089Z"
    }
   },
   "source": [
    "def create_mask(src, tgt,device=DEVICE):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n\n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n\n    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional encoding\n",
    "The transformer model doesn't have built-in knowledge of the order of tokens in the sequence. To give the model this information, positional encodings are added to the tokens embeddings. These encodings have a fixed pattern based on their position in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.778430Z",
     "start_time": "2025-08-20T16:20:14.774557Z"
    }
   },
   "source": [
    "# Add positional information to the input tokens\nclass PositionalEncoding(nn.Module):\n    def __init__(self,\n                 emb_size: int,\n                 dropout: float,\n                 maxlen: int = 5000):\n        super(PositionalEncoding, self).__init__()\n        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n        pos_embedding = torch.zeros((maxlen, emb_size))\n        pos_embedding[:, 0::2] = torch.sin(pos * den)\n        pos_embedding[:, 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding', pos_embedding)\n\n    def forward(self, token_embedding: Tensor):\n        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token embedding\n",
    "Token embedding, also known as word embedding or word representation, is a way to convert words or tokens from a text corpus into numerical vectors in a continuous vector space. Each unique word or token in the corpus is assigned a fixed-length vector where the numerical values represent various linguistic properties of the word, such as its meaning, context, or relationships with other words.\n",
    "\n",
    "The `TokenEmbedding` class below converts numerical tokens into embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.805403Z",
     "start_time": "2025-08-20T16:20:14.800831Z"
    }
   },
   "source": [
    "class TokenEmbedding(nn.Module):\n    def __init__(self, vocab_size: int, emb_size):\n        super(TokenEmbedding, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        self.emb_size = emb_size\n\n    def forward(self, tokens: Tensor):\n        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.827113Z",
     "start_time": "2025-08-20T16:20:14.822900Z"
    }
   },
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n    def __init__(self,\n                 num_encoder_layers: int,\n                 num_decoder_layers: int,\n                 emb_size: int,\n                 nhead: int,\n                 src_vocab_size: int,\n                 tgt_vocab_size: int,\n                 dim_feedforward: int = 512,\n                 dropout: float = 0.1):\n        super(Seq2SeqTransformer, self).__init__()\n\n        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n        self.positional_encoding = PositionalEncoding(\n            emb_size, dropout=dropout)\n        self.transformer = Transformer(d_model=emb_size,\n                                       nhead=nhead,\n                                       num_encoder_layers=num_encoder_layers,\n                                       num_decoder_layers=num_decoder_layers,\n                                       dim_feedforward=dim_feedforward,\n                                       dropout=dropout)\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                src_mask: Tensor,\n                tgt_mask: Tensor,\n                src_padding_mask: Tensor,\n                tgt_padding_mask: Tensor,\n                memory_key_padding_mask: Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n        outs =outs.to(DEVICE)\n        return self.generator(outs)\n\n    def encode(self, src: Tensor, src_mask: Tensor):\n        return self.transformer.encoder(self.positional_encoding(\n                            self.src_tok_emb(src)), src_mask)\n\n    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n                          self.tgt_tok_emb(tgt)), memory,\n                          tgt_mask)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "\n",
    "The diagram below illustrates the sequence prediction or inference process.\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/predict_transformers.png\" alt=\"transformer\">\n",
    "The decoder's output is then mapped onto a vocabulary-sized vector using a linear layer. Following this, a softmax function converts these vector scores into probabilities. The highest probability, as determined by the argmax function, provides the index of your predicted word within the translated sequence. This predicted index is fed back into the decoder in conjunction with the initial sequence, setting the stage to determine the subsequent word in the translation. This autoregressive process is demonstrated by the arrow pointing to form the top of the decoder, in green, to the bottom.\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:14.850015Z",
     "start_time": "2025-08-20T16:20:14.847417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add this after the get_translation_dataloaders_hf function\n",
    "vocab_transform = {}\n",
    "\n",
    "def create_vocab_transform(train_dataloader):\n",
    "    \"\"\"Create vocab_transform dictionary for compatibility\"\"\"\n",
    "    global vocab_transform\n",
    "    vocab_transform = {\n",
    "        'de': train_dataloader.de_vocab,\n",
    "        'en': train_dataloader.en_vocab\n",
    "    }\n",
    "    return vocab_transform"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:16.648345Z",
     "start_time": "2025-08-20T16:20:14.869844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader, _ = get_translation_dataloaders_hf(batch_size=1)\n",
    "set_global_vocabs(train_dataloader)\n",
    "\n",
    "# Create vocab_transform for compatibility with existing code\n",
    "vocab_transform = create_vocab_transform(train_dataloader)\n",
    "\n",
    "# Now your existing code will work\n",
    "SRC_LANGUAGE = 'de'\n",
    "TGT_LANGUAGE = 'en'\n",
    "SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\n",
    "TGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n",
    "EMB_SIZE = 512\n",
    "\n",
    "print(f\"Source (German) vocab size: {SRC_VOCAB_SIZE}\")\n",
    "print(f\"Target (English) vocab size: {TGT_VOCAB_SIZE}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabularies...\n",
      "English vocab size: 7964\n",
      "German vocab size: 9762\n",
      "Source (German) vocab size: 9762\n",
      "Target (English) vocab size: 7964\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:16.963916Z",
     "start_time": "2025-08-20T16:20:16.664868Z"
    }
   },
   "source": [
    "torch.manual_seed(0)\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\nSRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 128\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n\nfor p in transformer.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n\ntransformer = transformer.to(DEVICE)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's will start off with a trained model.For this, load the weights of the transformer model from the file 'transformer.pt'.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T16:20:17.432430Z",
     "start_time": "2025-08-20T16:20:16.984062Z"
    }
   },
   "source": [
    "transformer.load_state_dict(torch.load('transformer.pt', map_location=DEVICE, ))"
   ],
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2SeqTransformer:\n\tsize mismatch for src_tok_emb.embedding.weight: copying a param with shape torch.Size([19214, 512]) from checkpoint, the shape in current model is torch.Size([9762, 512]).\n\tsize mismatch for tgt_tok_emb.embedding.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.bias: copying a param with shape torch.Size([10837]) from checkpoint, the shape in current model is torch.Size([7964]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtransformer.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mDEVICE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/lib/python3.12/site-packages/torch/nn/modules/module.py:2593\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2585\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2586\u001B[0m             \u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   2587\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2588\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)\n\u001B[1;32m   2589\u001B[0m             ),\n\u001B[1;32m   2590\u001B[0m         )\n\u001B[1;32m   2592\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2593\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m   2594\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2595\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)\n\u001B[1;32m   2596\u001B[0m         )\n\u001B[1;32m   2597\u001B[0m     )\n\u001B[1;32m   2598\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Seq2SeqTransformer:\n\tsize mismatch for src_tok_emb.embedding.weight: copying a param with shape torch.Size([19214, 512]) from checkpoint, the shape in current model is torch.Size([9762, 512]).\n\tsize mismatch for tgt_tok_emb.embedding.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.weight: copying a param with shape torch.Size([10837, 512]) from checkpoint, the shape in current model is torch.Size([7964, 512]).\n\tsize mismatch for generator.bias: copying a param with shape torch.Size([10837]) from checkpoint, the shape in current model is torch.Size([7964])."
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"engish target\",index_to_eng(tgt))\n",
    "#print(\"german input\",index_to_german(src))"
   ],
   "execution_count": null
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "f583ab330d392f3fbc803e1d84830f575a94e0d7cc0f8b3af49ded45fd51cc14"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
